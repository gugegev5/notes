# 深度学习常用优化方法

作者：LogM

本文原载于 [https://segmentfault.com/u/logm/articles](https://segmentfault.com/u/logm/articles) ，不允许转载~

文章中的数学公式若无法正确显示，请参见：[正确显示数学公式的小技巧](https://segmentfault.com/a/1190000019359797)

## 1. SGD（随机梯度下降）

$$
g_t = \bigtriangledown_{\theta_{t-1}} f(\theta_{t-1})
$$

$$
\Delta\theta_t = -\eta*g_t
$$

- 需要手动选取合适的learning_rate

- 稀疏特征更新问题（不常出现的特征更新慢，如word2vec的embedding矩阵）

- 易被困局部最优

## 2. momentum

$$
m_t = \mu*m_{t-1}+g_t
$$

$$
\Delta\theta_t = -\eta*m_t
$$

- 引入惯量缓解局部最优问题

- 未解决稀疏特征更新问题

## 3. AdaGrad

$$\nu_t = \nu_{t-1} + g_t*g_t$$

$$\Delta\theta_t = \frac{g_t}{\sqrt{\nu_t+\epsilon}} * \eta$$

- 解决稀疏特征更新问题，不常出现的特征更新快

- 局部最优问题没有解决

- 随着 $\nu_t$ 的累加，学习率不断衰减

## 4. RMSprop

$$\nu_t = \mu * \nu_{t-1} + (1-\mu) * g_t*g_t$$

$$\Delta\theta_t = \frac{g_t}{\sqrt{\nu_t+\epsilon}} * \eta$$

- 解决稀疏特征更新问题，不常出现的特征更新快

- 局部最优问题没有解决

> RMSprop 是 AdaGrad 的升级版，区别是 $\nu_t$ 的计算方式：RMSprop 是移动平均，而 AdaGrad 是累加，越加越大。

## 5. AdaDelta

$$\nu_t = \mu * \nu_{t-1} + (1-\mu) * g_t*g_t$$

$$\Delta\theta_t = \frac{g_t}{\sqrt{\nu_t+\epsilon}} * \sqrt{\Delta \hat\theta_{t-1}^2}$$

$$\hat\theta_{t}^2 = \mu * \hat\theta_{t-1}^2 + (1-\mu) * \Delta\theta_t * \Delta\theta_t$$

- 解决稀疏特征更新问题，不常出现的特征更新快

- 无需手动设置学习率

- 局部最优问题没有解决

- > AdaDelta 相比 RMSprop，使用了 $\sqrt{\Delta \hat\theta_{t-1}^2}$ 来替换 $\eta$。

## 4. Adam

$$
m_t = \beta_{1}m_{t-1} + (1-\beta_1)g_t
$$

$$
\nu_t = \beta_{2}\nu_{t-1} + (1-\beta_2)g_t^2
$$

$$
\Delta\theta_t = \frac{\hat m_t}{\sqrt{\hat\nu_t+\epsilon}} * \eta
$$

$$
\hat m_t = \frac{m_t}{1-\beta_1^t},\space\space \hat\nu_t = \frac{\nu_t}{1-\beta_2^t}
$$

- $m_t$：移动平均求一阶矩的期望$E(g_t)$

- $\nu_t$：移动平均求二阶矩的期望$E(g_t^2)$

- $\hat m_t$、$\hat\nu_t$：考虑迭代刚开始的初始时刻做的修正

- 解决稀疏特征更新问题

- 缓解局部最优问题

> Adam 可以看做是将 momentum 和 RMSprop 的思想融合了起来。