# 交叉熵的推导

作者：LogM

本文原载于 [https://segmentfault.com/u/logm/articles](https://segmentfault.com/u/logm/articles) ，不允许转载~

文章中的数学公式若无法正确显示，请参见：[正确显示数学公式的小技巧](https://segmentfault.com/a/1190000019359797)

## 1. 第一种方式

用极大似然估计法推导。

设：

$$\pi(x) = P(Y=1|x)$$

$$1-\pi(x) = P(Y=0|x)$$

所以，对于训练集 $T=\{(x_1,y_1), (x_2,y_2), ..., (x_N,y_N)\}$ 有似然函数：

$$\prod_{i=1}^{N}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$

化成对数形式：

$$L(w) = \sum_{i=1}^{N}[y_i log (\pi(x_i))+(1-y_i)log(1-\pi(x_i))]$$

就是我们熟悉的交叉熵。

## 2. 第二种方式

从信息论角度推导。

### 2.1 信息熵

一个事件 $x$ 发生的概率为 $p(x)$，其发生时携带的信息量为：

$$I(x) = -log(p(x))$$

我们把信息量的期望叫做"熵"：

$$H(X) = -\sum_{i=1}^{N}p(x_i)log(p(x_i))$$

### 2.2 相对熵

我们可以用"相对熵"（或称KL散度）来衡量两个分布的距离。在机器学习中，$P$ 往往用来表示样本的真实分布，$Q$ 是我们训练模型得到的分布。当 $Q$ 的分布与 $P$ 相同时，它们的距离为 $0$。

$$D_{KL}(p||q) = \sum_{i=1}^{N}p(x_i)log(\frac{p(x_i)}{q(x_i)})$$

### 2.3 交叉熵

化开相对熵的公式，可以有：

$$D_{KL}(p||q) = \sum_{i=1}^{N}p(x_i)log(\frac{p(x_i)}{q(x_i)})$$

$$ = \sum_{i=1}^{N}p(x_i)log(p(x_i)) - \sum_{i=1}^{N}p(x_i)log(q(x_i)) = -H(P) - \sum_{i=1}^{N}p(x_i)log(q(x_i))$$

机器学习中，$H(P)$ 是真实 label 确定的，不会改变。我们只需要关心右边，我们把右边记为"交叉熵"：

$$H(p,q) = - \sum_{i=1}^{N}p(x_i)log(q(x_i))$$